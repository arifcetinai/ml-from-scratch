{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f83b579",
   "metadata": {},
   "source": [
    "# Linear Regression — From Scratch\n",
    "\n",
    "Gradient Descent, Mini-Batch, and Normal Equation implemented with **NumPy only**.\n",
    "\n",
    "See [math.md](./math.md) for the full mathematical derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from linear_regression.linear_regression import LinearRegression\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "print(\"Libraries loaded ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bdccd6",
   "metadata": {},
   "source": [
    "## 1. Load & Prepare Data — California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24615e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing(as_frame=False)\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_train.shape}  |  Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75382b",
   "metadata": {},
   "source": [
    "## 2. Train — Batch Gradient Descent vs Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9393d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch Gradient Descent ---\n",
    "gd_model = LinearRegression(learning_rate=0.1, n_iterations=500, method=\"gradient_descent\")\n",
    "gd_model.fit(X_train, y_train)\n",
    "gd_r2 = gd_model.score(X_test, y_test)\n",
    "\n",
    "# --- Normal Equation ---\n",
    "ne_model = LinearRegression(method=\"normal_equation\")\n",
    "ne_model.fit(X_train, y_train)\n",
    "ne_r2 = ne_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Gradient Descent  →  R² = {gd_r2:.4f}\")\n",
    "print(f\"Normal Equation   →  R² = {ne_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db266e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Loss Curve ---\n",
    "axes[0].plot(gd_model.loss_history, color=\"royalblue\", lw=2)\n",
    "axes[0].set_title(\"Training Loss (MSE) — Gradient Descent\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"MSE\")\n",
    "\n",
    "# --- Predictions vs Actuals ---\n",
    "y_pred = gd_model.predict(X_test)\n",
    "axes[1].scatter(y_test, y_pred, alpha=0.3, s=10, color=\"steelblue\")\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2, label=\"Perfect fit\")\n",
    "axes[1].set_title(f\"Predicted vs Actual  (R²={gd_r2:.3f})\")\n",
    "axes[1].set_xlabel(\"Actual\")\n",
    "axes[1].set_ylabel(\"Predicted\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
